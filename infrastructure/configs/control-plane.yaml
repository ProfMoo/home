machine:
  nodeLabels:
    drmoo.io/role: ${node_type}
    drmoo.io/zone: ${proxmox_node}
  certSANs:
    - ${ipv4_address}/24
    - "kubernetes.default.svc.cluster.local"
    - 127.0.0.1
    - ${talos_virtual_ip}
  kubelet:
    defaultRuntimeSeccompProfileEnabled: true
    disableManifestsDirectory: false
    extraArgs:
      rotate-server-certificates: true
  network:
    hostname: ${hostname}
    nameservers:
      - ${subnet_gateway}
    interfaces:
      - interface: ${interface}
        addresses:
          - ${ipv4_address}/24
        routes:
          - network: 0.0.0.0/0
            gateway: ${subnet_gateway}
        vip:
          ip: ${talos_virtual_ip}
  features:
    # https://www.talos.dev/v1.7/talos-guides/network/host-dns/
    hostDNS:
      enabled: true
      resolveMemberNames: true
    kubePrism:
      enabled: true
      port: 7445
    kubernetesTalosAPIAccess:
      enabled: true
      allowedRoles:
        - os:reader
      allowedKubernetesNamespaces:
        - kube-system
  install:
    # NOTE: Adjust this image to either 1. install new extensions and/or 2. upgrade Talos
    image: factory.talos.dev/installer/dc7b152cb3ea99b821fcb7340ce7168313ce393d663740b791c36f6e95fc8586:v1.7.0
  files:
    - op: overwrite
      path: /etc/nfsmount.conf
      permissions: 0o644
      content: |
        [ NFSMount_Global_Options ]
        nfsvers=4.1
        hard=True
        noatime=True
        nodiratime=True
        rsize=131072
        wsize=131072
        nconnect=8
cluster:
  network:
    podSubnets: ${format("%#v",split(",",pod_subnets))}
    serviceSubnets: ${format("%#v",split(",",service_subnets))}
    cni:
      name: custom
      urls:
        - https://raw.githubusercontent.com/ProfMoo/home/master/infrastructure/manifests/cilium.yaml
  apiServer:
    extraArgs:
      feature-gates: ServerSideApply=true
      # For the life of me, I cannot figure out why I need to set this field.
      #
      # Problem: I have been dealing with persistent inconsistency wrt Kubernetes pods authentication to the Kubernetes API server.
      #   This would usually show up when a new pod starts up - it would attempt to connect to the API server dozens of times before finally succeeding.
      #   It also affected Prometheus, as the Prometheus pod could only ever scrape *some* of the cluster components.
      #
      # Investigation: The tokens that were being loaded into the Kubernetes pods would have the `aud` field of the JWT data be only one control plane node.
      #   Ex: { "aud": "https://192.168.8.20:6443" }
      #   This node would be the node that the API server who was asked for a token is running on.
      #   The "aud" field determines *who* the token is meant for. Meaning that the token is self-limiting and therefore only authenticates to API server pods running on that particular node.
      #   This doesn't match what other Talos/homelab users see on their tokens (or anyone else I can find on the internet)
      #   Most users would have all the control plane nodes (like I've done below) or a load-balanced endpoint across all the nodes.
      #   The weirdest part is that I've been unable to figure out *why* my configuration is different from everyone else.
      #
      # Workaround: I manually define the control plane node IPs here, so that the aud field has all the API server pod's nodes
      #     https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
      #   This is still not perfect, as some pods are displaying weird behavior (like for example now we can't scrape metrics from the API servers???)
      #   Hopefully at some point this all makes sense. The best guess I have right now has to do with the `externalCloudProvider` field that I set during Talos startup.
      #     https://www.talos.dev/v1.7/reference/configuration/v1alpha1/config/#Config.cluster.externalCloudProvider
      #   Perhaps this flag, which I use to apply configuration to the cluster, actually enables some specific configuration to interact with cloud providers.
      #   Namely, this might tell the cluster to set specific values so that the external cloud provider can control authentication, similar to this article:
      #     https://www.talos.dev/v1.7/advanced/machine-config-oauth/#configuration
      api-audiences: "https://192.168.8.20:6443,https://192.168.8.21:6443,https://192.168.8.22:6443"
    resources:
      requests:
        cpu: 500m
        memory: 500Mi
      limits:
        cpu: 1
        memory: 1Gi
  proxy:
    # Using Cilium as the proxy, so disabling kube-proxy
    disabled: true
  etcd:
    extraArgs:
      # NOTE: We need to override the default bind address so that the /metrics endpoint is able to be scraped by Prometheus
      # Further discussion: https://github.com/siderolabs/talos/discussions/7799
      listen-metrics-urls: http://0.0.0.0:2381
  controllerManager:
    extraArgs:
      # NOTE: Kubernetes manifests controlled by Talos require a special application process compared to most machine-config changes.
      # Source: https://github.com/siderolabs/talos/discussions/7835
      bind-address: 0.0.0.0
    resources:
      requests:
        cpu: 500m
        memory: 500Mi
      limits:
        cpu: 1
        memory: 1Gi
  scheduler:
    extraArgs:
      # NOTE: This enables Prometheus to scrape metrics from the control plane nodes
      bind-address: 0.0.0.0
    resources:
      requests:
        cpu: 500m
        memory: 500Mi
      limits:
        cpu: 1
        memory: 1Gi
  inlineManifests:
    - name: fluxcd
      contents: |-
        apiVersion: v1
        kind: Namespace
        metadata:
            name: flux-system
            labels:
              app.kubernetes.io/instance: flux-system
              app.kubernetes.io/part-of: flux
              pod-security.kubernetes.io/warn: restricted
              pod-security.kubernetes.io/warn-version: latest
    - name: cilium
      contents: |-
        apiVersion: v1
        kind: Namespace
        metadata:
            name: cilium
            labels:
              pod-security.kubernetes.io/enforce: "privileged"
  externalCloudProvider:
    enabled: true
    manifests:
      # NOTE: We include the cert-approval in the bootstrap manifests since it enables viewing pod logs.
      # If the cluster startup fails for any reason, it can be difficult to debug without viewing logs.
      - https://raw.githubusercontent.com/ProfMoo/home/master/infrastructure/manifests/cert-approval.yaml
      - https://raw.githubusercontent.com/ProfMoo/home/master/infrastructure/manifests/flux-initialization.yaml
      - https://raw.githubusercontent.com/ProfMoo/home/master/infrastructure/manifests/flux-repo-bootstrap.yaml
